{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "handmade_decision_tree.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GerardJLarkin/MachineLearning/blob/master/handmade_decision_tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKGN5uvUsF6a"
      },
      "source": [
        "## Manual development of a Decision Tree learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDn7YYlPsN71"
      },
      "source": [
        "# upgrade gspread: only need to run if you run into authorization issues\n",
        "#!pip install --upgrade gspread"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3pb7h_ZsOS3"
      },
      "source": [
        "# authorise the importing of data from my own google drive\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())\n",
        "\n",
        "worksheet = gc.open('beer.txt').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "#print(rows)\n",
        "\n",
        "# Convert to a DataFrame and render.\n",
        "import pandas as pd\n",
        "d1 = pd.DataFrame(rows)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from operator import itemgetter\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXSEIHkSvNyH"
      },
      "source": [
        "# add column/feature headings to dataset\n",
        "# this is a manual step required with our dataset\n",
        "d1.columns = ['calorific_value', 'nitrogen', 'turbidity', 'style', 'alcohol', \\\n",
        "              'sugars', 'bitterness', 'beer_id', 'colour', \\\n",
        "              'degree_of_fermentation']\n",
        "#print(type(d1))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWQBLaL2wLm_"
      },
      "source": [
        "# put target column to the end of the dataframe for ease of access\n",
        "# this is a manual step required for our dataset\n",
        "d1 = d1[['calorific_value','nitrogen', 'turbidity', 'alcohol','sugars', \\\n",
        "         'bitterness', 'beer_id', 'colour', 'degree_of_fermentation','style']]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSbO4SkMDd92"
      },
      "source": [
        "## Preprocessing steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2BgihznMyTr"
      },
      "source": [
        "# After reviewing the dataset I identified the column beer_id as a feature of unique values\n",
        "# These will not be great for decision making as there is no relationship within the feature\n",
        "# that corresponds to the different beer styles, it will therefore be removed\n",
        "d1 = d1[['calorific_value','nitrogen', 'turbidity', 'alcohol','sugars', \\\n",
        "         'bitterness','colour', 'degree_of_fermentation','style']]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1pDv53zzz8c"
      },
      "source": [
        "# split data into train and test sets\n",
        "train=d1.sample(frac=0.66667, random_state=1)\n",
        "test=d1.drop(train.index)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PkgPcCJFz0Iv"
      },
      "source": [
        "# split training and test data into features and target\n",
        "X_train = train.iloc[:,:-1].astype(float)\n",
        "y_train = pd.DataFrame(train.iloc[:,-1]).astype('category')\n",
        "\n",
        "X_test = test.iloc[:,:-1].astype(float)\n",
        "y_test = pd.DataFrame(test.iloc[:,-1]).astype('category')\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWFVQp6PF3Gb"
      },
      "source": [
        "# select k best features by getting the variance in each feature\n",
        "# it is important to note that scaling a feature also changes its variance so \n",
        "# it's necessary to determine variance on the non-scaled dataset to see which\n",
        "# (if any) feature is appropriate to remove\n",
        "col_nam = list(X_train.columns)\n",
        "col_vars = list(X_train.var())\n",
        "col_nam_var = list(zip(col_nam, col_vars))\n",
        "# set variance threshold at arbirary value of 0.5\n",
        "column_name = [item[0] for item in col_nam_var if item[1] > 0.5] \n",
        "X_train = X_train[column_name]\n",
        "\n",
        "# select the same features for the test dataset\n",
        "X_test = X_test[column_name]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yUbq3GJz0Tw"
      },
      "source": [
        "# scale each feature in the training & tests sets to remove bias on dominant \n",
        "# features\n",
        "X_train_norm = (X_train - X_train.min()) / (X_train.max() - X_train.min())\n",
        "X_test_norm = (X_test - X_test.min()) / (X_test.max() - X_test.min())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUNBSV7Sm_3J"
      },
      "source": [
        "## Build the Decision Tree Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asfbv9WL0BAr"
      },
      "source": [
        "# create function to split dataset on maximum gain ratio\n",
        "# logic for this function derived from explanation of algorithm at following link\n",
        "# https://sefiks.com/2018/05/13/a-step-by-step-c4-5-decision-tree-example/\n",
        "def gain_ratio_split(features, target):\n",
        "    # calculate entropy of target labels before split\n",
        "    target_ent = -sum((target.value_counts()/len(target)) * np.log2((target.value_counts()/len(target))))\n",
        "\n",
        "    gainratios = []\n",
        "    num_cols = features.shape[1]\n",
        "    # get index of columns\n",
        "    for i in range(num_cols):\n",
        "        # get column name via its index\n",
        "        column_name = features.columns[i]\n",
        "        # call input dataframe again and get column to perform entropy calc on\n",
        "        single_column_df = pd.DataFrame(features[column_name])\n",
        "        # add target lables to dataframe to calucluate entropy\n",
        "        duo_column_df = single_column_df.merge(target, left_index=True, right_index=True).sort_values(by=column_name)\n",
        "        # get number of rows to create iterative sub frames to perform comnbined entropy over\n",
        "        for j in range(len(duo_column_df)):\n",
        "            # this call gets the number of rows starting at the beginning corresponding to interative index\n",
        "            sub_df_1 = duo_column_df.head(j)\n",
        "            # this call gets the remainder of the dataframe excluded from the previous call\n",
        "            sub_df_2 = duo_column_df.tail(len(duo_column_df)-j)\n",
        "            # sense check if either sub df is empty then pass as errors will occur\n",
        "            if sub_df_1.empty or sub_df_2.empty:\n",
        "                pass\n",
        "            else:\n",
        "                # find the mid point value between the two subsets (max of lower subset, min of upper subset)\n",
        "                mp_val = ((sub_df_1.max(axis = 0)[0] + sub_df_2.min(axis = 0)[0]) / 2)  \n",
        "                # calculate entropy for both sub frames\n",
        "                sub_df_1_ent = -sum(sub_df_1.iloc[:,1].value_counts()/len(sub_df_1) * np.log2(sub_df_1.iloc[:,1].value_counts()/len(sub_df_1)))\n",
        "                sub_df_2_ent = -sum(sub_df_2.iloc[:,1].value_counts()/len(sub_df_2) * np.log2(sub_df_2.iloc[:,1].value_counts()/len(sub_df_2)))\n",
        "                # calculate information gain by from each of the sub frame splits\n",
        "                gain = target_ent - ((len(sub_df_1)/len(duo_column_df)) * sub_df_1_ent) - ((len(sub_df_2)/len(duo_column_df)) * sub_df_2_ent)\n",
        "                # calculate split info to normalise gain (remove bias)\n",
        "                splitinfo = -( (len(sub_df_1)/len(duo_column_df)) * np.log2( (len(sub_df_1)/len(duo_column_df)) )) - \\\n",
        "                             ( (len(sub_df_2)/len(duo_column_df)) * np.log2( (len(sub_df_2)/len(duo_column_df)) ))\n",
        "                # gain ratio calculation \n",
        "                gainratio = np.nan_to_num(gain / splitinfo)\n",
        "                gainratios.append((column_name, mp_val, gainratio))\n",
        "    # get feature, feature value and IG to perform split on by getting max gain ratio across all features\n",
        "    try:\n",
        "        out = max(gainratios,key=itemgetter(2))\n",
        "    except:\n",
        "        out = (None, None, None)\n",
        "\n",
        "    return out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htN6Y3V1gDJJ"
      },
      "source": [
        "# create function to calculate node impurity\n",
        "def node_impurity(node_data):\n",
        "    imp_val = list(node_data.iloc[:,-1].value_counts()/ len(node_data))\n",
        "    # if the length of imp_val is greater than 1 it contains multiple classes\n",
        "    if len(imp_val) == 1:\n",
        "        return True\n",
        "    else:\n",
        "        return False"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3fvK1x6h1SW"
      },
      "source": [
        "def leaf_classifier(node_data):\n",
        "    return node_data.iloc[:,-1].value_counts().idxmax()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZf_WB-JtZWd"
      },
      "source": [
        "# After multiple iterations I found that a min tree depth of 6 returns the highest\n",
        "# accuracy, above 6 the accuracy does not increase. my model does not run with a \n",
        "# depth of 5, 2, or 1. depth of 4 and 3 have a lower accuracy than 6\n",
        "def dec_t(features, target, depth = 0, stop = 6):\n",
        "    stop = stop\n",
        "    target = target\n",
        "    depth = depth  \n",
        "    data = features.merge(target, left_index = True, right_index = True)\n",
        "    n_imp = node_impurity(data)\n",
        "\n",
        "    # check to see if we are at final recursion or if node is pure, if so classify node\n",
        "    if (n_imp == True) or (depth == stop) or (features.empty):\n",
        "        return leaf_classifier(data)\n",
        "    # If the data is not pure or we are still within the max number of recursions:\n",
        "    else:\n",
        "        # call split function to get feature and value to split on\n",
        "        column_name, mp_val, gainratio = gain_ratio_split(features, target)\n",
        "        if column_name is None:\n",
        "            return leaf_classifier(data)\n",
        "        else:\n",
        "            # create string of feature & split values to design tree\n",
        "            split = (\"{} {}\").format(column_name, mp_val)\n",
        "            # create dictionary to append edges and their values\n",
        "            edge = {split : []}\n",
        "            depth += 1\n",
        "            # get child datasets after split\n",
        "            # there is a question as to whether or not to remove the feature that the data is being split\n",
        "            # on so it will not be available for the next iteration. If I do not drop them my algorthim\n",
        "            # continuously uses the same feature but splits at different values, if I exclude my algorithm\n",
        "            # choses a new feature each time. \n",
        "            # I am choosing to drop a feature at each recursion: https://machinelearningmastery.com/rfe-feature-selection-in-python/\n",
        "            child1 = features[features[column_name] >= mp_val].drop(columns = column_name)\n",
        "            child2 = features[features[column_name] < mp_val].drop(columns = column_name)\n",
        "            # get apprpritate target values for new child datasets\n",
        "            tar1 = pd.DataFrame(child1.merge(target, left_index=True, right_index=True).iloc[:,-1])\n",
        "            tar2 = pd.DataFrame(child2.merge(target, left_index=True, right_index=True).iloc[:,-1])\n",
        "            # begin recursion and append recursive output to edges dictionary to print tree\n",
        "            left = dec_t(child1, tar1, depth = depth, stop = stop)\n",
        "            right = dec_t(child2, tar2, depth = depth, stop = stop)\n",
        "            if left != right:\n",
        "                edge[split].append(left)\n",
        "                edge[split].append(right)\n",
        "            \n",
        "            return edge\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMbcGv490kql",
        "outputId": "7c8aa27a-3db8-4bb1-d44c-611fbee24c56"
      },
      "source": [
        "from pprint import pprint\n",
        "t = dec_t(X_train_norm, y_train)\n",
        "pprint(t)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'bitterness 0.3445410409184757': [{'calorific_value 0.015075376570541452': [{'turbidity 0.017860609564024174': [{'sugars 0.07331975560081494': [{'colour 0.2938144329896908': [{'degree_of_fermentation 0.2946390462061134': ['ale',\n",
            "                                                                                                                                                                                                                               'lager']},\n",
            "                                                                                                                                                                                'lager']},\n",
            "                                                                                                                                                 'lager']},\n",
            "                                                                                                                 'lager']},\n",
            "                                                                             'lager']},\n",
            "                                   {'calorific_value 0.05276381913257297': [{'turbidity 0.026014366078588928': [{'sugars 0.018329938900203645': [{'colour 0.03608247422680412': [{'degree_of_fermentation 0.10252397352502358': ['stout',\n",
            "                                                                                                                                                                                                                                 'lager']},\n",
            "                                                                                                                                                                                 'stout']},\n",
            "                                                                                                                                                 'lager']},\n",
            "                                                                                                                'lager']},\n",
            "                                                                            'stout']}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yn5U6r6M0FS"
      },
      "source": [
        "def unseen(data, tree):\n",
        "    # extract the first key of the tree dictionary I built in the previous\n",
        "    # function\n",
        "    edge = list(tree.keys())[0]\n",
        "    # extract feature and split value\n",
        "    feature, mp_val = edge.split(sep = ' ')\n",
        "    # classify unseen data based on same criteria as decision tree function\n",
        "    if data[feature] >= float(mp_val):\n",
        "        # check if extraced key is still in dict format, if so apply function again\n",
        "        # if not we've reached our label\n",
        "        if isinstance(tree[edge][0], dict) == True:\n",
        "            return unseen(data, tree[edge][0])\n",
        "        else:\n",
        "            return tree[edge][0]\n",
        "    else:\n",
        "        if isinstance(tree[edge][1], dict) == True:\n",
        "            return unseen(data, tree[edge][1])\n",
        "        else:\n",
        "            return tree[edge][1]\n",
        "\n",
        "def predictor(data, tree):\n",
        "    # In the docs its states the args need to be a tuple, if I leave one argument (tree) \n",
        "    # its treated like a str so after many attempts I have to insert the comma to give\n",
        "    # a null second element of a tuple\n",
        "    data['style'] = data.apply(unseen, axis = 1, args = (tree, ))\n",
        "\n",
        "    return data\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWxb9vL50QKa"
      },
      "source": [
        "# create a copy of the test feature set\n",
        "test_cases = X_test_norm.copy()\n",
        "# merge it with the test target set to get real labels\n",
        "test_with_label = test_cases.merge(y_test, left_index = True, right_index = True)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVZuyeo7tsgb"
      },
      "source": [
        "# predict target labels from predictor function\n",
        "pred = predictor(test_cases, t)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "u6fMlzdL1SU9",
        "outputId": "031814ce-1d01-4468-8c65-5e3345260a67"
      },
      "source": [
        "# find accuracy of model prediction\n",
        "correct_labels = test_with_label.merge(pred, how = 'inner', suffixes = ('t_', 'p_'))\n",
        "accuracy = '{:.2f}'.format(len(correct_labels)/len(test_with_label) * 100) + '%'\n",
        "accuracy"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'70.59%'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jwCAZ3FyHMO"
      },
      "source": [
        "# cross validate model by selecting 10 random subsets of test data\n",
        "def rand_pred_test(test, tree):\n",
        "    accuracy_values = []\n",
        "    for i in range(10):\n",
        "        sub_df = test.sample(frac = 0.2)\n",
        "        test_with_label = sub_df.merge(y_test, left_index = True, right_index = True)\n",
        "        pred = predictor(sub_df, tree)\n",
        "        correct_labels = test_with_label.merge(pred, how = 'inner', suffixes = ('t_', 'p_'))\n",
        "        accuracy = len(correct_labels)/len(test_with_label)\n",
        "        accuracy_values.append(accuracy)\n",
        "\n",
        "    return np.mean(accuracy_values)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Y2eDfTbL1ggZ",
        "outputId": "78b02287-0a04-4f20-d0b9-49fb9afadfc8"
      },
      "source": [
        "# get average accuracy of model after predicting 10 subsets of the test dataset\n",
        "# as the subsets change each time, the average accuracy changes after each run\n",
        "avg_accuracy = '{:.2f}'.format(rand_pred_test(X_test_norm, t) * 100) + '%'\n",
        "avg_accuracy"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'72.00%'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AuuNqdIcD5HX",
        "outputId": "57fd68a3-2b56-4f6e-bef8-9da777ea6918"
      },
      "source": [
        "# choose reference algorithm to assess accuracy of handmade model\n",
        "# as I coded up an implementation of te C4.5 algorithm, the closest algorithim\n",
        "# I could find in the sci-kit leanrn libraries was the CART decision tree classifier\n",
        "# I will implement this for comparision\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "classifier = DecisionTreeClassifier()\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14  1  0]\n",
            " [ 2 17  1]\n",
            " [ 0  0 16]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         ale       0.88      0.93      0.90        15\n",
            "       lager       0.94      0.85      0.89        20\n",
            "       stout       0.94      1.00      0.97        16\n",
            "\n",
            "    accuracy                           0.92        51\n",
            "   macro avg       0.92      0.93      0.92        51\n",
            "weighted avg       0.92      0.92      0.92        51\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}